# -*- coding: utf-8 -*-
"""notebookb9c97f0c30 (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nYOF2myOqC64s-dEXyCFZzRtrh-Q83oC
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import svm
# %matplotlib inline
from scipy.stats import mstats
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.ensemble import GradientBoostingClassifier
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

"""# Data Cleaning

"""

df=pd.read_csv("/kaggle/input/new-loan/Loan_Data.csv")
df

df.info()

df.isnull().sum()

df.Gender.fillna(df.Gender.mode()[0],inplace = True)

df['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace=True)

df['Married'].fillna(df['Married'].mode()[0], inplace=True)

df.Dependents.fillna(df['Dependents'].mode()[0], inplace=True)

df.LoanAmount.fillna(df.LoanAmount.mean(),inplace = True)

df.Loan_Amount_Term.fillna(df.Loan_Amount_Term.mode()[0], inplace=True)

df.Credit_History.fillna(df.Credit_History.mode()[0],inplace = True)

print(df.dtypes)

df.isnull().sum()

df.duplicated().sum()

"""# FEATURE ENGINEERING

**How much is the total income of someone who takes a loan?**
"""

df['TotalIncome']=df['ApplicantIncome']+df['CoapplicantIncome']
df['TotalIncome'].hist(bins=20)

"""**IT IS SKEWED DATA, SO WE NEED TO APPLY LOG**"""

df['TotalIncome']=np.log(df['TotalIncome'])
df['TotalIncome'].hist(bins=20)

df['LoanAmount'].hist(bins=20)

"""**IT IS SKEWED DATA, SO WE NEED TO APPLY LOG**"""

df['LoanAmount']=np.log(df['LoanAmount'])
df['LoanAmount'].hist(bins=20)

"""# UNIVARIATE ANALYSIS

**How many the number of people who take a loan as group by marital status?**
"""

sns.countplot(x='Married',data=df);

"""**How many the number of people who take a loan as group by gender?**"""

sns.countplot(x='Gender',data=df);

"""**How many the number of people who take loan by dependents?**"""

sns.countplot(x='Dependents',data=df);

"""**How many the number of people who take loan are self employed?**"""

sns.countplot(x='Self_Employed',data=df);

"""**How many the number of people who take loan have a credit history?**"""

sns.countplot(x='Credit_History',data=df);

df.boxplot(column=['ApplicantIncome'])
plt.show()

df.boxplot(column=['CoapplicantIncome'])
plt.show()

df.boxplot(column=['LoanAmount'])
plt.show()

df.boxplot(column=['Loan_Amount_Term'])
plt.show()

df.boxplot(column=['Credit_History'])
plt.show()

df["ApplicantIncome"].describe()

columns_to_handle = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']

# Apply Winsorization to handle outliers for each column
for column in columns_to_handle:
    win_data = mstats.winsorize(df[column], limits=[0.09, 0.09])
    df[column] = win_data

# Print the updated DataFrame
print(df.head())

df.boxplot(column=['ApplicantIncome'])
plt.show()

df.boxplot(column=['CoapplicantIncome'])
plt.show()

df.boxplot(column=['LoanAmount'])
plt.show()

df.boxplot(column=['Loan_Amount_Term'])
plt.show()

df.boxplot(column=['Credit_History'])
plt.show()

"""# Exploratory Data Analysis(EDA)

### data description
"""

df.info()

df.describe()

"""### Data Visualization

**How many people have a credit history in a bank?**
"""

plt.hist(data=df,x='Credit_History', bins=20, edgecolor='black')
plt.xlabel('Credit_History')
plt.ylabel('credithistory_counts')
plt.title('Distribution of Credit History')
plt.show()

"""**Which area needs more loans?**"""

df['Property_Area'].value_counts()
plt.hist(data=df,x='Property_Area', bins=5, edgecolor='black')
plt.xlabel('Property Area')
plt.ylabel('Frequency')
plt.title('Distribution of Property Area')
plt.show()

"""**How much is the total income of someone who takes a loan?**"""

plt.hist(data=df,x='TotalIncome', bins=10, edgecolor='black')
plt.xlabel('Total Income')
plt.ylabel('Amount')
plt.title('Distribution of Total Incomes')
plt.show()

"""# BIVARIATE ANALYSIS

**How much is the total income of people who took a loan, and how much loan amount did they take?**
"""

plt.scatter(data = df, x = 'TotalIncome', y = 'LoanAmount');
plt.xlabel('Total Income')
plt.ylabel('Loan Amount')
plt.title('Relationship between Total Income and Loan Amount')
plt.show()

"""**How much is the total income of people who took a loan, and do they have a credit history in a bank or not?**"""

plt.scatter(data = df, x = 'Credit_History', y = 'TotalIncome');
plt.xlabel('Credit History')
plt.ylabel('Total Income')
plt.title('Relationship between Total Income and Credit_History')
plt.show()

"""**Which area needs more loans, and how much is the average loan amount they take?**"""

sns.barplot(data=df, x='Property_Area', y='LoanAmount', ci=None, color='skyblue')  # ci=None removes error bars

plt.xlabel('Property Area')
plt.ylabel('Average Loan Amount')
plt.title('Average Loan Amount by Property Area')

plt.show()

"""**How many people graduated taking a loan from a bank or not?**"""

cross_tab = pd.crosstab(df['Education'], df['Loan_Status'])
print(cross_tab)

sns.heatmap(pd.crosstab(df.Education, df.Loan_Status), annot=True, fmt='d');

sns.countplot(data=df, x='Education', hue='Loan_Status', palette='Set2')

plt.xlabel('Education Level')
plt.ylabel('Count')
plt.title('Relationship between Education and Loan Status')

plt.show()

"""**How many people self employed taking a loan from a bank or not?**"""

cross_tab = pd.crosstab(df['Self_Employed'], df['Loan_Status'])
print(cross_tab)

sns.heatmap(pd.crosstab(df.Self_Employed, df.Loan_Status), annot=True, fmt='d');

sns.countplot(data=df, x='Self_Employed', hue='Loan_Status', palette='Set3')

plt.xlabel('Self Employment Status')
plt.ylabel('Count')
plt.title('Relationship between Self Employment and Loan Status')

plt.show()

"""# MULTIVARIATE ANALYSIS

**What is the relation or correlation between all numerical columns?**
"""

numerical_columns = ['TotalIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']

correlation_matrix = df[numerical_columns].corr()

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""# MACHINE LEARNING PREPROCESSING

# ENCODING
"""

from sklearn import preprocessing

# lable encoders
label_encoder = preprocessing.LabelEncoder()

# converting gender to numeric values
df['Gender'] = label_encoder.fit_transform(df['Gender'])
df['Married'] = label_encoder.fit_transform(df['Married'])
df['Self_Employed'] = label_encoder.fit_transform(df['Self_Employed'])
df['Loan_Status'] = label_encoder.fit_transform(df['Loan_Status'])
df['Education'] = label_encoder.fit_transform(df['Education'])
df['Property_Area'] = label_encoder.fit_transform(df['Property_Area'])
df['Dependents']=label_encoder.fit_transform(df['Dependents'])
df.head()

df.info()

"""# SCALING

# DecisionTreeClassifier, RandomForestClassifier, and GradientBoostingClassifier do not require feature scaling or normalization. This is because:They split data based on feature thresholds, which are not affected by the feature's scale.The model is invariant to scaling because it only depends on feature ordering, not their magnitude.

# SPLITTING DATA
"""

# Separate features and target variable
X = df.drop(['Loan_ID', 'Loan_Status'], axis=1)
y = df['Loan_Status']

#Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model 1
# DecisionTree

"""**accuracy in train test split **"""

#Create and train the Decision Tree classifier
pipelineDecisionTree = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing
    ('classifier', DecisionTreeClassifier())  # Model
])

# Train pipeline on training set
pipelineDecisionTree.fit(X_train, y_train)

# Evaluate on the test set
y_pred_dt = pipelineDecisionTree.predict(X_test)
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Accuracy: {accuracy_dt:.2f}")

print("Predicted labels:", y_pred_dt)

model = DecisionTreeClassifier(random_state=42)

# Fit the model
model.fit(X_train, y_train)

# Visualize the Decision Tree
plt.figure(figsize=(15, 10))
plot_tree(model, feature_names=X_train.columns, filled=True)
plt.show()

#cross validation accuracy of DecisionTreeClassifier with k folds split

import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import cross_val_score, KFold
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification

# Create a synthetic dataset for binary classification
X, y = make_classification(n_samples=100, n_features=5, random_state=42)

# Define the pipeline
pipelineDecisionTree = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing
    ('classifier', DecisionTreeClassifier(random_state=42))  # Model
])

# Perform K-fold cross-validation (e.g., 5-fold)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
cv_scores_dt = cross_val_score(pipelineDecisionTree, X, y, cv=kf)

# Print cross-validation results
print(f"Cross-validation accuracy scores for each fold: {cv_scores_dt}")
print(f"Mean accuracy across all folds: {cv_scores_dt.mean():.2f}")

# If you want to train the final model and visualize the decision tree
pipelineDecisionTree.fit(X, y)

# Visualize the Decision Tree
plt.figure(figsize=(15, 10))
plot_tree(pipelineDecisionTree.named_steps['classifier'], feature_names=[f"Feature {i}" for i in range(X.shape[1])], filled=True)
plt.show()

# Model 2
# Naive Bayes

# Define pipeline
pipelineGaussianNB = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', GaussianNB())
])

# Evaluate pipeline using cross-validation
scores_nb = cross_val_score(pipelineGaussianNB, X, y, cv=5)
print(f"Cross-validation accuracy: {scores_nb.mean():.2f}")

nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)
y_prednb = nb_classifier.predict(X_test)

accuracy_GaussianNB = accuracy_score(y_test, y_prednb)
print("Accuracy:", accuracy_GaussianNB)

# Make predictions on the test set
y_prednb = nb_classifier.predict(X_test)

# Print the predicted labels
print("Predicted labels:", y_prednb)

#Naive Bayes model with k-fold split

from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB

# Define pipeline
pipelineGaussianNB = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step
    ('classifier', GaussianNB())  # Model
])

# Define k-fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold CV

# Perform cross-validation scoring
scores_nb2 = cross_val_score(pipelineGaussianNB, X, y, cv=kf)
print(f"Cross-validation accuracy scores for each fold: {scores_nb2}")
print(f"Mean CV accuracy: {scores_nb2.mean():.2f}")

# Perform cross-validation predictions
y_prednb2 = cross_val_predict(pipelineGaussianNB, X, y, cv=kf)

# Evaluate predictions
accuracy_GaussianNB = accuracy_score(y, y_prednb2)
print(f"Overall accuracy (from cross-validated predictions): {accuracy_GaussianNB:.4f}")
print("\nClassification Report:")
print(classification_report(y, y_prednb2))

# Model 3
# Gradient Boosting

"""**accuracy GradientBoostingClassifier with k-folds split**"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# Create the pipeline
pipeline_gb = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step
    ('model', GradientBoostingClassifier(random_state=42))  # Model
])

# Define k-fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold CV

# Perform cross-validation scoring
cv_scores_gb = cross_val_score(pipeline_gb, X, y, cv=kf)  # Evaluates pipeline

# Print cross-validation results
print(f"Cross-validation accuracy scores for each fold: {cv_scores_gb}")
print(f"Mean CV accuracy: {np.mean(cv_scores_gb):.4f}")

# Perform cross-validation predictions
y_pred_gb = cross_val_predict(pipeline_gb, X, y, cv=kf)  # Get predictions for each fold

# Evaluate predictions
accuracy_gb = accuracy_score(y, y_pred_gb)
print(f"Overall accuracy (from cross-validated predictions): {accuracy_gb:.4f}")
print("\nClassification Report:")
print(classification_report(y, y_pred_gb))

"""**accuracy GradientBoostingClassifier with train test split**"""

pipeline_gb = Pipeline([
    ('model', GradientBoostingClassifier(random_state=42))  # Step 2: Apply GradientBoostingClassifier
])

# Perform cross-validation
cv_scores_gb2 = cross_val_score(pipeline_gb, X_train, y_train, cv=5)  # 5-fold cross-validation

# Print cross-validation results
print(f"Cross-validation scores: {cv_scores_gb2}")
print(f"Mean CV score: {np.mean(cv_scores_gb2):.4f}")

from sklearn.metrics import accuracy_score
# Fit the pipeline using training data
pipeline_gb.fit(X_train, y_train)
# Predict using the fitted pipeline
y_pred_gb2 = pipeline_gb.predict(X_test)

print("Predicted labels:", y_pred_gb2)

"""**Accuracy without cross validation**"""

# Calculate the accuracy of the model
accuracy_gb2 = accuracy_score(y_test, y_pred_gb2)
print("Accuracy:", accuracy_gb2)

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Access an individual tree
gb_model = pipeline_gb.named_steps['model']  # Extract the GradientBoostingClassifier
tree = gb_model.estimators_[0, 0]  # Access the first tree of the first stage

# Plot the tree
plt.figure(figsize=(20, 10))
plot_tree(tree, filled=True, feature_names=X_train.columns, class_names=[str(cls) for cls in np.unique(y_train)], fontsize=10)
plt.title("Tree 1 in Gradient Boosting Classifier")
plt.show()

"""
**model 4:  Neural Network with test_train split**
s"""

# Scale the numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the Neural Network architecture
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32)

# Evaluate the model on the testing data
loss, accuracy = model.evaluate(X_test, y_test)
print('Test Loss:', loss)
print('Test Accuracy:', accuracy)

from tensorflow.keras.utils import plot_model
# Plot the model architecture
plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)
plt.figure(figsize=(10, 10))
img = plt.imread('model_architecture.png')
plt.imshow(img)
plt.axis('off')
plt.show()

"""**Neural Network with a pipeline with StandardScaler and custom Keras model and with K-fold split**"""

from sklearn.base import BaseEstimator, ClassifierMixin
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
import numpy as np

# Define a custom estimator class
class KerasNNClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, epochs=50, batch_size=10):
        self.epochs = epochs
        self.batch_size = batch_size
        self.model = None

    def fit(self, X, y):
        self.model = Sequential()
        self.model.add(Dense(64, input_dim=X.shape[1], activation='relu'))  # First hidden layer
        self.model.add(Dense(64, activation='relu'))  # Second hidden layer
        self.model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification
        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

        # Fit the model
        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)
        return self

    def predict(self, X):
        return (self.model.predict(X) > 0.5).astype(int)

# Create a pipeline with StandardScaler and custom Keras model
pipeline_nn = Pipeline([
    ('scaler', StandardScaler()),  # Step 1: Scale the data
    ('model', KerasNNClassifier(epochs=50, batch_size=10))  # Step 2: Custom neural network model
])

# Now you can use this pipeline with KFold as shown earlier
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Store results
cv_scores_nn2 = []

# Perform KFold cross-validation
for train_index, val_index in kf.split(X):
    # Split data into training and validation sets
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # Standardize the data
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)

    # Create and train the model (assuming create_model is defined)
    model = create_model()
    model.fit(X_train_scaled, y_train, epochs=50, batch_size=10, verbose=0)

    # Evaluate the model
    y_pred_nn2 = (model.predict(X_val_scaled) > 0.5).astype(int)
    accuracy_nn2 = accuracy_score(y_val, y_pred_nn2)
    cv_scores_nn2.append(accuracy_nn2)

# Print cross-validation results
print(f"Cross-validation scores: {cv_scores_nn2}")
print(f"Mean cross-validation score: {np.mean(cv_scores_nn2):.4f}")







