# -*- coding: utf-8 -*-
"""notebookb9c97f0c30.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X6KTYisB0-FZgTsxrSYVhBrN-0vhJm0i
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import svm
# %matplotlib inline
from scipy.stats import mstats
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.ensemble import GradientBoostingClassifier
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

"""# Data Cleaning

"""

df=pd.read_csv("/kaggle/input/new-loan/Loan_Data.csv")
df

df.info()

df.isnull().sum()

df.Gender.fillna(df.Gender.mode()[0],inplace = True)

df['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace=True)

df['Married'].fillna(df['Married'].mode()[0], inplace=True)

df.Dependents.fillna(df['Dependents'].mode()[0], inplace=True)

df.LoanAmount.fillna(df.LoanAmount.mean(),inplace = True)

df.Loan_Amount_Term.fillna(df.Loan_Amount_Term.mode()[0], inplace=True)

df.Credit_History.fillna(df.Credit_History.mode()[0],inplace = True)

print(df.dtypes)

df.isnull().sum()

df.duplicated().sum()

"""# FEATURE ENGINEERING

**How much is the total income of someone who takes a loan?**
"""

df['TotalIncome']=df['ApplicantIncome']+df['CoapplicantIncome']
df['TotalIncome'].hist(bins=20)

"""**IT IS SKEWED DATA, SO WE NEED TO APPLY LOG**"""

df['TotalIncome']=np.log(df['TotalIncome'])
df['TotalIncome'].hist(bins=20)

df['LoanAmount'].hist(bins=20)

"""**IT IS SKEWED DATA, SO WE NEED TO APPLY LOG**"""

df['LoanAmount']=np.log(df['LoanAmount'])
df['LoanAmount'].hist(bins=20)

"""# UNIVARIATE ANALYSIS

**How many the number of people who take a loan as group by marital status?**
"""

sns.countplot(x='Married',data=df);

"""**How many the number of people who take a loan as group by gender?**"""

sns.countplot(x='Gender',data=df);

"""**How many the number of people who take loan by dependents?**"""

sns.countplot(x='Dependents',data=df);

"""**How many the number of people who take loan are self employed?**"""

sns.countplot(x='Self_Employed',data=df);

"""**How many the number of people who take loan have a credit history?**"""

sns.countplot(x='Credit_History',data=df);

df.boxplot(column=['ApplicantIncome'])
plt.show()

df.boxplot(column=['CoapplicantIncome'])
plt.show()

df.boxplot(column=['LoanAmount'])
plt.show()

df.boxplot(column=['Loan_Amount_Term'])
plt.show()

df.boxplot(column=['Credit_History'])
plt.show()

df["ApplicantIncome"].describe()

columns_to_handle = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']

# Apply Winsorization to handle outliers for each column
for column in columns_to_handle:
    win_data = mstats.winsorize(df[column], limits=[0.09, 0.09])
    df[column] = win_data

# Print the updated DataFrame
print(df.head())

df.boxplot(column=['ApplicantIncome'])
plt.show()

df.boxplot(column=['CoapplicantIncome'])
plt.show()

df.boxplot(column=['LoanAmount'])
plt.show()

df.boxplot(column=['Loan_Amount_Term'])
plt.show()

df.boxplot(column=['Credit_History'])
plt.show()

"""# Exploratory Data Analysis(EDA)

### data description
"""

df.info()

df.describe()

"""### Data Visualization

**How many people have a credit history in a bank?**
"""

plt.hist(data=df,x='Credit_History', bins=20, edgecolor='black')
plt.xlabel('Credit_History')
plt.ylabel('credithistory_counts')
plt.title('Distribution of Credit History')
plt.show()

"""**Which area needs more loans?**"""

df['Property_Area'].value_counts()
plt.hist(data=df,x='Property_Area', bins=5, edgecolor='black')
plt.xlabel('Property Area')
plt.ylabel('Frequency')
plt.title('Distribution of Property Area')
plt.show()

"""**How much is the total income of someone who takes a loan?**"""

plt.hist(data=df,x='TotalIncome', bins=10, edgecolor='black')
plt.xlabel('Total Income')
plt.ylabel('Amount')
plt.title('Distribution of Total Incomes')
plt.show()

"""# BIVARIATE ANALYSIS

**How much is the total income of people who took a loan, and how much loan amount did they take?**
"""

plt.scatter(data = df, x = 'TotalIncome', y = 'LoanAmount');
plt.xlabel('Total Income')
plt.ylabel('Loan Amount')
plt.title('Relationship between Total Income and Loan Amount')
plt.show()

"""**How much is the total income of people who took a loan, and do they have a credit history in a bank or not?**"""

plt.scatter(data = df, x = 'Credit_History', y = 'TotalIncome');
plt.xlabel('Credit History')
plt.ylabel('Total Income')
plt.title('Relationship between Total Income and Credit_History')
plt.show()

"""**Which area needs more loans, and how much is the average loan amount they take?**"""

sns.barplot(data=df, x='Property_Area', y='LoanAmount', ci=None, color='skyblue')  # ci=None removes error bars

plt.xlabel('Property Area')
plt.ylabel('Average Loan Amount')
plt.title('Average Loan Amount by Property Area')

plt.show()

"""**How many people graduated taking a loan from a bank or not?**"""

cross_tab = pd.crosstab(df['Education'], df['Loan_Status'])
print(cross_tab)

sns.heatmap(pd.crosstab(df.Education, df.Loan_Status), annot=True, fmt='d');

sns.countplot(data=df, x='Education', hue='Loan_Status', palette='Set2')

plt.xlabel('Education Level')
plt.ylabel('Count')
plt.title('Relationship between Education and Loan Status')

plt.show()

"""**How many people self employed taking a loan from a bank or not?**"""

cross_tab = pd.crosstab(df['Self_Employed'], df['Loan_Status'])
print(cross_tab)

sns.heatmap(pd.crosstab(df.Self_Employed, df.Loan_Status), annot=True, fmt='d');

sns.countplot(data=df, x='Self_Employed', hue='Loan_Status', palette='Set3')

plt.xlabel('Self Employment Status')
plt.ylabel('Count')
plt.title('Relationship between Self Employment and Loan Status')

plt.show()

"""# MULTIVARIATE ANALYSIS

**What is the relation or correlation between all numerical columns?**
"""

numerical_columns = ['TotalIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']

correlation_matrix = df[numerical_columns].corr()

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""# MACHINE LEARNING PREPROCESSING

# ENCODING
"""

from sklearn import preprocessing

# lable encoders
label_encoder = preprocessing.LabelEncoder()

# converting gender to numeric values
df['Gender'] = label_encoder.fit_transform(df['Gender'])
df['Married'] = label_encoder.fit_transform(df['Married'])
df['Self_Employed'] = label_encoder.fit_transform(df['Self_Employed'])
df['Loan_Status'] = label_encoder.fit_transform(df['Loan_Status'])
df['Education'] = label_encoder.fit_transform(df['Education'])
df['Property_Area'] = label_encoder.fit_transform(df['Property_Area'])
df['Dependents']=label_encoder.fit_transform(df['Dependents'])
df.head()

df.info()

"""# SCALING

# DecisionTreeClassifier, RandomForestClassifier, and GradientBoostingClassifier do not require feature scaling or normalization. This is because:They split data based on feature thresholds, which are not affected by the feature's scale.The model is invariant to scaling because it only depends on feature ordering, not their magnitude.

# SPLITTING DATA
"""

# Separate features and target variable
X = df.drop(['Loan_ID', 'Loan_Status'], axis=1)
y = df['Loan_Status']

#Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model 1
# DecisionTree

#Create and train the Decision Tree classifier
pipelineDecisionTree = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing
    ('classifier', DecisionTreeClassifier())  # Model
])

# Train pipeline on training set
pipelineDecisionTree.fit(X_train, y_train)

# Evaluate on the test set
y_pred = pipelineDecisionTree.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("Predicted labels:", y_pred)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

model = DecisionTreeClassifier(random_state=42)

# Fit the model
model.fit(X_train, y_train)

# Visualize the Decision Tree
plt.figure(figsize=(15, 10))
plot_tree(model, feature_names=X_train.columns, filled=True)
plt.show()
save_path = '/kaggle/working/ML_Model.pkl'
with open(save_path, 'wb') as file:
    pickle.dump(model, file)

print(f"Model saved to {save_path}")
# Model 2
# Naive Bayes

# Define pipeline
pipelineGaussianNB = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', GaussianNB())
])

# Evaluate pipeline using cross-validation
scores = cross_val_score(pipelineGaussianNB, X, y, cv=5)
print(f"Cross-validation accuracy: {scores.mean():.2f}")

nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)
y_prednb = nb_classifier.predict(X_test)

accuracy_GaussianNB = accuracy_score(y_test, y_prednb)
print("Accuracy:", accuracy_GaussianNB)

# Make predictions on the test set
y_prednb = nb_classifier.predict(X_test)

# Print the predicted labels
print("Predicted labels:", y_prednb)

# visual

# Model 3
# Gradient Boosting

pipeline_gb = Pipeline([
    ('model', GradientBoostingClassifier(random_state=42))  # Step 2: Apply GradientBoostingClassifier
])

# Perform cross-validation
cv_scores = cross_val_score(pipeline_gb, X_train, y_train, cv=5)  # 5-fold cross-validation

# Print cross-validation results
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean CV score: {np.mean(cv_scores):.4f}")

from sklearn.metrics import accuracy_score
# Fit the pipeline using training data
pipeline_gb.fit(X_train, y_train)
# Predict using the fitted pipeline
y_pred_gb = pipeline_gb.predict(X_test)

print("Predicted labels:", y_pred_gb)

"""**Accuracy without cross validation**"""

# Calculate the accuracy of the model
accuracy_gb = accuracy_score(y_test, y_pred_gb)
print("Accuracy:", accuracy_gb)

# visual

# model 4
# Neural Networks
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import KFold
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# One-hot encode the target variable for neural network classification
y_encoded = to_categorical(y, num_classes=3)

# Define the neural network model
def create_model():
    model = Sequential()
    model.add(Dense(64, input_dim=X.shape[1], activation='relu'))  # First hidden layer
    model.add(Dense(64, activation='relu'))  # Second hidden layer
    model.add(Dense(3, activation='softmax'))  # Output layer for multi-class classification
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Initialize KFold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Store results
cv_scores = []

# Perform KFold cross-validation
for train_index, val_index in kf.split(X_scaled):
    # Split data into training and validation sets
    X_train, X_val = X_scaled[train_index], X_scaled[val_index]
    y_train, y_val = y_encoded[train_index], y_encoded[val_index]

    # Create and train the model
    model = create_model()
    model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)

    # Evaluate the model on the validation set
    y_pred = model.predict(X_val)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_val_classes = np.argmax(y_val, axis=1)

    # Calculate accuracy and store the result
    accuracy = accuracy_score(y_val_classes, y_pred_classes)
    cv_scores.append(accuracy)

# Print cross-validation results
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean cross-validation score: {np.mean(cv_scores):.4f}")

